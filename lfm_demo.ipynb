{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFM (Liquid Foundation Models) Demo\n",
    "\n",
    "This notebook demonstrates how to use the Liquid Foundation Models (LFM) for text generation tasks.\n",
    "\n",
    "## Features\n",
    "- Load pre-trained LFM models\n",
    "- Generate text with different sampling strategies\n",
    "- Fine-tune models on custom data\n",
    "- Visualize model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers accelerate sentencepiece protobuf\n",
    "!pip install matplotlib seaborn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load LFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection\n",
    "model_options = {\n",
    "    \"1B\": \"liquid/lfm-1b\",\n",
    "    \"3B\": \"liquid/lfm-3b\", \n",
    "    \"7B\": \"liquid/lfm-7b\",\n",
    "    \"13B\": \"liquid/lfm-13b\",\n",
    "    \"40B\": \"liquid/lfm-40b\"\n",
    "}\n",
    "\n",
    "# Select model size based on available memory\n",
    "model_size = \"3B\"  # Change this based on your hardware\n",
    "model_name = model_options[model_size]\n",
    "\n",
    "print(f\"Loading {model_name}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Generation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, \n",
    "                  max_length: int = 100,\n",
    "                  temperature: float = 0.7,\n",
    "                  top_p: float = 0.9,\n",
    "                  top_k: int = 50,\n",
    "                  num_return_sequences: int = 1) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate text using the LFM model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        max_length: Maximum length of generated text\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "        top_p: Nucleus sampling parameter\n",
    "        top_k: Top-k sampling parameter\n",
    "        num_return_sequences: Number of sequences to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of generated text sequences\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(text)\n",
    "    \n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Once upon a time in a digital world\",\n",
    "    \"The most important scientific discovery of the 21st century is\",\n",
    "    \"To solve climate change, we need to\"\n",
    "]\n",
    "\n",
    "# Generate text for each prompt\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    generated = generate_text(prompt, max_length=150, temperature=0.8)\n",
    "    print(generated[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation with parameter controls\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "@interact(\n",
    "    prompt=widgets.Textarea(value='The key to happiness is', description='Prompt:'),\n",
    "    max_length=widgets.IntSlider(min=50, max=500, step=50, value=150, description='Max Length:'),\n",
    "    temperature=widgets.FloatSlider(min=0.1, max=2.0, step=0.1, value=0.7, description='Temperature:'),\n",
    "    top_p=widgets.FloatSlider(min=0.1, max=1.0, step=0.1, value=0.9, description='Top-p:'),\n",
    "    top_k=widgets.IntSlider(min=10, max=100, step=10, value=50, description='Top-k:')\n",
    ")\n",
    "def interactive_generate(prompt, max_length, temperature, top_p, top_k):\n",
    "    result = generate_text(prompt, max_length, temperature, top_p, top_k)[0]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze generation with different temperatures\n",
    "test_prompt = \"The meaning of life is\"\n",
    "temperatures = [0.3, 0.5, 0.7, 0.9, 1.2]\n",
    "results = []\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated = generate_text(test_prompt, max_length=100, temperature=temp, num_return_sequences=3)\n",
    "    results.append({\n",
    "        'temperature': temp,\n",
    "        'generations': generated\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"\\nTemperature: {result['temperature']}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, gen in enumerate(result['generations']):\n",
    "        print(f\"Sample {i+1}: {gen[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token probabilities\n",
    "def get_token_probabilities(prompt: str, next_tokens: int = 10):\n",
    "    \"\"\"\n",
    "    Get probabilities for the next tokens after a prompt.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "    # Get top k tokens and their probabilities\n",
    "    top_probs, top_indices = torch.topk(probs, next_tokens)\n",
    "    top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "    \n",
    "    return top_tokens, top_probs.cpu().numpy()\n",
    "\n",
    "# Visualize token probabilities\n",
    "prompt = \"The weather today is\"\n",
    "tokens, probs = get_token_probabilities(prompt, next_tokens=15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(tokens)), probs)\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "plt.xlabel('Next Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Token Probabilities after: \"{prompt}\"')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different sampling strategies\n",
    "strategies = [\n",
    "    {\"name\": \"Greedy\", \"temperature\": 0.1, \"top_p\": 1.0, \"top_k\": 1},\n",
    "    {\"name\": \"Low Temperature\", \"temperature\": 0.5, \"top_p\": 0.9, \"top_k\": 50},\n",
    "    {\"name\": \"Medium Temperature\", \"temperature\": 0.8, \"top_p\": 0.9, \"top_k\": 50},\n",
    "    {\"name\": \"High Temperature\", \"temperature\": 1.2, \"top_p\": 0.95, \"top_k\": 100},\n",
    "    {\"name\": \"Nucleus Sampling\", \"temperature\": 0.7, \"top_p\": 0.7, \"top_k\": 0}\n",
    "]\n",
    "\n",
    "prompt = \"In the year 2050, technology will\"\n",
    "strategy_results = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    generated = generate_text(\n",
    "        prompt, \n",
    "        max_length=100,\n",
    "        temperature=strategy[\"temperature\"],\n",
    "        top_p=strategy[\"top_p\"],\n",
    "        top_k=strategy[\"top_k\"] if strategy[\"top_k\"] > 0 else 50000\n",
    "    )[0]\n",
    "    \n",
    "    strategy_results.append({\n",
    "        \"Strategy\": strategy[\"name\"],\n",
    "        \"Generated Text\": generated[len(prompt):].strip()[:100] + \"...\"\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(strategy_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tuning Example (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of preparing data for fine-tuning\n",
    "# Note: Actual fine-tuning requires more setup and computational resources\n",
    "\n",
    "# Sample training data\n",
    "training_examples = [\n",
    "    {\"input\": \"Translate to French: Hello, how are you?\", \"output\": \"Bonjour, comment allez-vous?\"},\n",
    "    {\"input\": \"Summarize: The quick brown fox jumps over the lazy dog.\", \"output\": \"A fox jumps over a dog.\"},\n",
    "    {\"input\": \"Question: What is the capital of France?\", \"output\": \"Answer: Paris\"},\n",
    "]\n",
    "\n",
    "# Format for fine-tuning\n",
    "formatted_data = []\n",
    "for example in training_examples:\n",
    "    text = f\"{example['input']}\\n{example['output']}\"\n",
    "    formatted_data.append(text)\n",
    "\n",
    "print(\"Sample formatted training data:\")\n",
    "for i, data in enumerate(formatted_data[:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure generation speed\n",
    "import time\n",
    "\n",
    "def measure_generation_speed(prompt: str, max_length: int = 100, num_runs: int = 5):\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = generate_text(prompt, max_length=max_length)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    tokens_per_second = max_length / avg_time\n",
    "    \n",
    "    return {\n",
    "        \"average_time\": avg_time,\n",
    "        \"tokens_per_second\": tokens_per_second,\n",
    "        \"all_times\": times\n",
    "    }\n",
    "\n",
    "# Test generation speed\n",
    "speed_results = measure_generation_speed(\"Once upon a time\", max_length=100)\n",
    "\n",
    "print(f\"Average generation time: {speed_results['average_time']:.2f} seconds\")\n",
    "print(f\"Tokens per second: {speed_results['tokens_per_second']:.2f}\")\n",
    "\n",
    "# Visualize generation times\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(speed_results['all_times'], marker='o')\n",
    "plt.axhline(y=speed_results['average_time'], color='r', linestyle='--', label='Average')\n",
    "plt.xlabel('Run Number')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Generation Time per Run')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Usage: Custom Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedGenerator:\n",
    "    \"\"\"\n",
    "    Generate text with custom constraints.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def generate_with_keywords(self, prompt: str, keywords: List[str], max_length: int = 100):\n",
    "        \"\"\"\n",
    "        Generate text that includes specific keywords.\n",
    "        \"\"\"\n",
    "        # This is a simplified example - real implementation would be more complex\n",
    "        base_generation = generate_text(prompt, max_length=max_length)[0]\n",
    "        \n",
    "        # Check which keywords are missing\n",
    "        missing_keywords = [kw for kw in keywords if kw.lower() not in base_generation.lower()]\n",
    "        \n",
    "        return {\n",
    "            \"generated_text\": base_generation,\n",
    "            \"included_keywords\": [kw for kw in keywords if kw.lower() in base_generation.lower()],\n",
    "            \"missing_keywords\": missing_keywords\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "generator = ConstrainedGenerator(model, tokenizer)\n",
    "result = generator.generate_with_keywords(\n",
    "    \"Write a story about\",\n",
    "    keywords=[\"dragon\", \"castle\", \"knight\"],\n",
    "    max_length=200\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(result[\"generated_text\"])\n",
    "print(f\"\\nIncluded keywords: {result['included_keywords']}\")\n",
    "print(f\"Missing keywords: {result['missing_keywords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Load Generated Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generation history\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "generation_history = []\n",
    "\n",
    "def save_generation(prompt: str, generated_text: str, parameters: dict):\n",
    "    \"\"\"\n",
    "    Save generation to history.\n",
    "    \"\"\"\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    generation_history.append(entry)\n",
    "\n",
    "# Generate and save some examples\n",
    "test_prompts = [\n",
    "    \"The secret to success is\",\n",
    "    \"In a world where AI can\",\n",
    "    \"The most beautiful thing about nature is\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    params = {\"temperature\": 0.7, \"max_length\": 100, \"top_p\": 0.9}\n",
    "    generated = generate_text(prompt, **params)[0]\n",
    "    save_generation(prompt, generated, params)\n",
    "\n",
    "# Display history\n",
    "print(f\"Generated {len(generation_history)} texts\")\n",
    "for i, entry in enumerate(generation_history):\n",
    "    print(f\"\\n{i+1}. {entry['prompt'][:50]}...\")\n",
    "    print(f\"   Generated: {entry['generated_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export history to file\n",
    "with open('generation_history.json', 'w') as f:\n",
    "    json.dump(generation_history, f, indent=2)\n",
    "\n",
    "print(\"Generation history saved to 'generation_history.json'\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "df_history = pd.DataFrame([\n",
    "    {\n",
    "        \"Prompt\": entry[\"prompt\"][:30] + \"...\",\n",
    "        \"Temperature\": entry[\"parameters\"][\"temperature\"],\n",
    "        \"Length\": len(entry[\"generated_text\"].split()),\n",
    "        \"Timestamp\": entry[\"timestamp\"][:19]\n",
    "    }\n",
    "    for entry in generation_history\n",
    "])\n",
    "\n",
    "df_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Loading and using LFM models\n",
    "- Various text generation strategies\n",
    "- Interactive generation with parameter controls\n",
    "- Performance analysis and visualization\n",
    "- Custom generation constraints\n",
    "\n",
    "Feel free to experiment with different prompts, parameters, and model sizes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}